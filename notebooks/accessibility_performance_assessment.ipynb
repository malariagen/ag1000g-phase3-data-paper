{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance assessment\n",
    "\n",
    "\n",
    "## Determine binary classification statistics \n",
    "\n",
    "For the v3 accessibility map, vs:\n",
    "- phase 2 accessibility map\n",
    "- permissive\n",
    "- restrictive\n",
    "- 50% random classifier\n",
    "\n",
    "based on:\n",
    "- Mendelian error in held out crosses (autosomes)\n",
    "- male het calls (`n_male_het`, `n_males_called`, `all_males_called`) will be used instead of `mendel_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import gcsfs\n",
    "import zarr\n",
    "import dask.array as da\n",
    "from dask_kubernetes import KubeCluster\n",
    "from dask.distributed import Client\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings:\n",
    "\n",
    "# Note that Mendelian errors (or preferably any data) from these crosses \n",
    "# should *not* have been used to compute the phase 2 accessibility map, otherwise it will not be a fair/accurate test.\n",
    "cross_family_ids = pd.read_csv(\"performance_eval_crosses.txt\", squeeze=True, header=None).values\n",
    "# Note that the accessibility resource contains data for `UNKN` and `Y_unplaced`\n",
    "chrom_arms = ['2R', '2L', '3R', '3L', 'X']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_family_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the cloud cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KubeCluster(n_workers=n_workers)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eyeball the name of this cluster, so we can identify associated worker pods\n",
    "cluster.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs = gcsfs.GCSFileSystem(project='malariagen-jupyterhub', token='cloud', cache_timeout=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine which crosses stats (inc. Mendelian error) are predicted by the phase 2 accessibility map\n",
    "\n",
    "Populate the confusion matrix, such that:\n",
    "- If the crosses stats imply that a position is accessible (\"good\") and the phase 2 map also says it is accessible, then mark the prediction as a `True Positive` (`TP`)\n",
    "- If the crosses stats imply that a position is accessible (\"good\") but the phase 2 map says it is inaccessible, then mark the prediction as a `False Negative` (`FN`)\n",
    "- If the crosses stats imply that a position is inaccessible (\"bad\") and the phase 2 map also says it is inaccessible, then mark the prediction as a `True Negative` (`TN`)\n",
    "- If the crosses stats imply that a position is inaccessible (\"bad\") but the phase 2 map says it is accessible, then mark the prediction as a `False Positive` (`FP`)\n",
    "- If the crosses stats cannot determine whether a position is \"good\" or \"bad\" (e.g. there were no male het calls on the X chromosome, but not all samples were called), then mark the prediction result as undetermined and exclude from subsequent stats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include results for artificial classifiers (for comparison)\n",
    "\n",
    "- random 50% is_accessible\n",
    "- 100% is_accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def cross_tab_values(eval_arr, test_arr):\n",
    "    \n",
    "    out = np.zeros((2, 2))\n",
    "    \n",
    "    # TN FP\n",
    "    # FN TP\n",
    "    \n",
    "    # eval arr has \n",
    "    for i in range(eval_arr.shape[0]):\n",
    "        if eval_arr[i] == -1:\n",
    "            continue\n",
    "        \n",
    "        if test_arr[i]:\n",
    "            out[eval_arr[i], 1] += 1\n",
    "        else:\n",
    "            out[eval_arr[i], 0] += 1\n",
    "    \n",
    "    return out.reshape((1, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define intermediate file storage\n",
    "zarr_access_out_path = Path(\n",
    "    \"vo_agam_production\", \"accessibility_maps\", \"v3\", \"accessibility_proposal\", \"gamb_colu\")\n",
    "\n",
    "store_access_out = gcsfs.mapping.GCSMap(zarr_access_out_path.as_posix())\n",
    "accessibility_out = zarr.group(store_access_out)\n",
    "\n",
    "def load_phase3_accessibility(chrom):\n",
    "    return da.from_zarr(accessibility_out[chrom][\"is_accessible\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessibility_gcsmap = gcsfs.mapping.GCSMap('ag1000g-release/phase2.AR1/accessibility/accessibility.zarr', gcs=gcs)\n",
    "accessibility_zarr = zarr.Group(accessibility_gcsmap, read_only=True)\n",
    "\n",
    "def load_phase2_accessibility(chrom):\n",
    "    \n",
    "    phase2_is_accessible = da.from_zarr(accessibility_zarr[chrom]['is_accessible'])\n",
    "    filter_n = da.from_zarr(accessibility_zarr[chrom]['filter_n'])\n",
    "    phase2_is_accessible_nonN = da.compress(~filter_n, phase2_is_accessible, axis=0)\n",
    "\n",
    "    # Compute chunk sizes to avoids this error when concatenating arrays: ValueError: Arrays chunk sizes are unknown: (nan,)\n",
    "    # This uses the cluster.\n",
    "    phase2_is_accessible_nonN.compute_chunk_sizes() \n",
    "    return phase2_is_accessible_nonN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosses_gcsmap = gcsfs.mapping.GCSMap('vo_agam_production/crosses_stats/v3', gcs=gcs)\n",
    "crosses_zarr = zarr.Group(crosses_gcsmap, read_only=True)\n",
    "\n",
    "def load_ground_truth(chrom, xid):\n",
    "    \n",
    "    if chrom == \"X\":\n",
    "        \n",
    "        # do something\n",
    "        any_het = da.from_zarr(crosses_zarr[chrom][\"n_male_het\"][xid]) > 0\n",
    "        is_all_called = da.from_zarr(crosses_zarr[chrom][\"all_males_called\"][xid])\n",
    "        \n",
    "        return da.where(any_het > 0, 0, da.where(is_all_called, 1, -1))\n",
    "\n",
    "    else:\n",
    "        me = da.from_zarr(crosses_zarr[chrom]['mendel_error'][xid])\n",
    "        called = da.from_zarr(crosses_zarr[chrom]['n_samples_called'][xid])\n",
    "    \n",
    "        return da.where(me > 0, 0, da.where(called == da.max(called), 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_sampler(size, frac_to_accept=0.5, chunks=\"auto\"):\n",
    "    \n",
    "    classif = da.random.random(size, chunks=chunks) < frac_to_accept\n",
    "    return classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_of_interest(true_neg, false_pos, false_neg, true_pos):\n",
    "    \n",
    "    r = {}\n",
    "    r[\"tpr\"] = true_pos / (true_pos + false_neg)\n",
    "    r[\"fpr\"] = false_pos / (false_pos + true_neg)\n",
    "    r[\"fdr\"] = false_pos / (true_pos + false_pos)\n",
    "    r[\"tnr\"] = 1 - (false_pos / (false_pos + true_neg))\n",
    "    r[\"youden\"] = r[\"tpr\"] + r[\"tnr\"] - 1\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrom_arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_family_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_chunks = (500_000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_holder = {}\n",
    "venn = {}\n",
    "\n",
    "for chrom_arm in chrom_arms:\n",
    "        \n",
    "    # varies by chrom only, but need to be chunked depending on ground_truth\n",
    "    alternative_calls = {\n",
    "        \"phase2\": load_phase2_accessibility(chrom_arm).rechunk(desired_chunks),\n",
    "        \"phase3_proposal\": load_phase3_accessibility(chrom_arm).rechunk(desired_chunks)\n",
    "    }\n",
    "    \n",
    "    alternative_calls[\"r0\"] = da.zeros(alternative_calls[\"phase2\"].shape[0], dtype=np.bool, chunks=desired_chunks)\n",
    "    alternative_calls[\"r50\"] = create_random_sampler(alternative_calls[\"phase2\"].shape[0], 0.5, desired_chunks)\n",
    "    alternative_calls[\"r100\"] = da.ones(alternative_calls[\"phase2\"].shape[0], dtype=np.bool, chunks=desired_chunks)\n",
    "    \n",
    "    chunks = ((1, ) * alternative_calls[\"phase2\"].numblocks[0], 2, 2)\n",
    "    x = da.map_blocks(\n",
    "        cross_tab_values, \n",
    "        alternative_calls[\"phase2\"].astype(\"int\"), \n",
    "        alternative_calls[\"phase3_proposal\"], \n",
    "        chunks=chunks,\n",
    "        dtype=np.int32,\n",
    "        new_axis=[1, 2]).sum(axis=0).compute()\n",
    "    \n",
    "    venn[chrom_arm] = x\n",
    "    \n",
    "    for cross_id in cross_family_ids:\n",
    "        \n",
    "        print(\"processing\", chrom_arm, cross_id)\n",
    "\n",
    "        # varies by cross and chrom\n",
    "        ground_truth = load_ground_truth(chrom_arm, cross_id).rechunk(desired_chunks)\n",
    "\n",
    "\n",
    "        for key, eval_data in alternative_calls.items():\n",
    "            \n",
    "            if (key, chrom_arm, cross_id) in data_holder:\n",
    "                print(\"skipping\", (key, chrom_arm, cross_id))\n",
    "                continue\n",
    "\n",
    "            chunks = ((1, ) * ground_truth.numblocks[0], 2, 2)\n",
    "            q = da.map_blocks(\n",
    "                cross_tab_values, \n",
    "                ground_truth, \n",
    "                eval_data, \n",
    "                chunks=chunks, \n",
    "                dtype=np.int32, \n",
    "                new_axis=[1, 2]).sum(axis=0)\n",
    "\n",
    "            (tn, fp), (fn, tp) = q.compute()\n",
    "\n",
    "            res = calculate_metrics_of_interest(tn, fp, fn, tp)\n",
    "            res[\"frac_accessible\"] = eval_data.mean().compute()\n",
    "\n",
    "            data_holder[key, chrom_arm, cross_id] = pd.Series(res, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "f, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "for ix, chrom in enumerate(chrom_arms):\n",
    "\n",
    "    d = np.round(venn[chrom] * 10e-6, 2)\n",
    "\n",
    "    access_p2 = d[1, 1] + d[1, 0]\n",
    "    access_p3 = d[0, 1] + d[1, 1]\n",
    "    access_both = d[1, 1]\n",
    "    access_p2_not_p3 = d[1, 0]\n",
    "    access_p3_not_p2 = d[0, 1]\n",
    "\n",
    "    i = ix // 3\n",
    "    j = ix % 3\n",
    "    venn2(subsets = (access_p2_not_p3, access_p3_not_p2, access_both), set_labels=(\"phase2\", \"v3_gamb_colu\"), ax=axes[i, j])\n",
    "    axes[i, j].set_title(chrom)\n",
    "\n",
    "    \n",
    "axes[-1, -1].axis('off')\n",
    "f.suptitle(\"Millions of shared sites\")\n",
    "f.savefig(\"shared_sites_p2_p3_venn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(data_holder, axis=0, sort=False, names=[\"description\", \"chrom\", \"cross_id\", \"metric\"])\n",
    "df.name = \"value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.pivot_table(\n",
    "    pd.DataFrame(df).reset_index(level=\"metric\"), \n",
    "    index=[\"description\", \"chrom\", \"cross_id\"], \n",
    "    columns=\"metric\", \n",
    "    values=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"performance_stats_all.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
