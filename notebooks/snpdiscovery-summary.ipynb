{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import allel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/dask/dataframe/utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import ag3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3 = ag3.release_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- how many segregating sites?\n",
    "\n",
    "- biallelic/multiallelic\n",
    "\n",
    "- how many undiscovered sites?\n",
    "\n",
    "## Reporting\n",
    "\n",
    "We want to report the headline number of SNPs. So that's gamb_colu snps with gamb_colu mask, plus arab snps with arab mask. \n",
    "Also report how many are private to arab and private to gamb_colu.\n",
    "Do this between gamb_colu vs arab and also gamb vs colu.\n",
    "\n",
    "Additionally for each species group, we want:\n",
    "n_seg sites, n_biallelic, n_multiallelic.\n",
    "\n",
    "## Definitions:\n",
    "Where there are different masks, cannot say for certain if private/shared.\n",
    "\n",
    "So, if a variant is seg in gamb_colu, and masked in arab, this does not count as private to gamb_colu.\n",
    "\n",
    "For each species generate 4 arrays: is_seg, is_multi, is_bial, is_masked.\n",
    "\n",
    "then the number of seg sites discovered is: `n_seg_sites = (is_seg & is_masked_).sum()`\n",
    "\n",
    "For the group comparisons:\n",
    "_shared_: segregating and accessible in both groups\n",
    "_private A_: segregating in A, not B. Accessible in both.\n",
    "_private B_: vv above\n",
    "_total_: segregating and accessible in _either_ group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask_kubernetes import KubeCluster\n",
    "from dask.distributed import Client, progress\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at:   tcp://10.34.4.153:41387\n",
      "distributed.scheduler - INFO -   dashboard at:                     :8787\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644ce93ad88f4c85b170d829f19023cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>KubeCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# kubernetes cluster setup\n",
    "n_workers = 30\n",
    "cluster = KubeCluster()\n",
    "cluster.scale_up(n_workers)\n",
    "#cluster.adapt(minimum=1, maximum=n_workers)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Receive client connection: Client-77d00c5a-8d7d-11eb-89e7-76b47941c236\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.34.4.153:41387</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/nicholasharding/proxy/8787/status' target='_blank'>/user/nicholasharding/proxy/8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.34.4.153:41387' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dask client setup\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosomes = \"2R\", \"2L\", \"3R\", \"3L\", \"X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sets = v3.all_sample_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register tcp://10.32.48.6:33085\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.48.6:33085\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.151.4:35483\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.151.4:35483\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.43.6:38595\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.43.6:38595\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.35.193.10:43779\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.35.193.10:43779\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.113.3:41445\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.113.3:41445\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.63.6:44047\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.63.6:44047\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.155.3:33621\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.155.3:33621\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.60.6:43407\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.60.6:43407\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.152.4:37659\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.152.4:37659\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.50.6:45547\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.50.6:45547\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.42.6:35077\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.42.6:35077\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.45.6:41283\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.45.6:41283\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.64.6:42009\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.64.6:42009\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.66.5:45791\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.66.5:45791\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.54.6:42017\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.54.6:42017\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.62.6:36587\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.62.6:36587\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.67.6:43617\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.67.6:43617\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.55.6:45985\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.55.6:45985\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.70.5:42785\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.70.5:42785\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.35.208.8:43639\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.35.208.8:43639\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.49.6:41121\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.49.6:41121\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.51.6:45913\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.51.6:45913\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.92.7:41671\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.92.7:41671\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.153.4:40237\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.153.4:40237\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.46.6:34683\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.46.6:34683\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.61.6:43103\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.61.6:43103\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.56.6:37683\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.56.6:37683\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.59.6:38797\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.59.6:38797\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.68.5:43315\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.68.5:43315\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.47.5:34113\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.47.5:34113\n",
      "distributed.core - INFO - Starting established connection\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/computation/expressions.py:68: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return op(a, b)\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/computation/expressions.py:68: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return op(a, b)\n"
     ]
    }
   ],
   "source": [
    "meta = v3.load_sample_set_metadata(sample_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3081, 18)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_definitions = {\n",
    "    \"gamb_colu\": \"species_gambcolu_arabiensis == 'gamb_colu'\",\n",
    "    \"arab\": \"species_gambcolu_arabiensis == 'arabiensis'\",\n",
    "    \"all\": \"species_gambcolu_arabiensis != 'NA'\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. All samples, g_c_a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_seg_df = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_product([pop_definitions.keys(), chromosomes]), \n",
    "    columns=[\"segregating\", \"multiallelic\", \"total\"],\n",
    "    dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allelism(block):\n",
    "\n",
    "    ac = allel.AlleleCountsArray(block)\n",
    "    \n",
    "    al  = ac.allelism()\n",
    "    \n",
    "    return al.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpops = meta.groupby(\"species_gambcolu_arabiensis\").indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_window_values(positions, loc, a, a_labels, path, path_total):\n",
    "    \n",
    "    arr_loc = np.compress(loc, a, axis=0)\n",
    "    pos_loc = allel.SortedIndex(np.compress(loc, positions, axis=0))\n",
    "    \n",
    "    eqa_windows = allel.stats.window.equally_accessible_windows(loc, size=100_000)\n",
    "    \n",
    "    # need to add the last window on.\n",
    "    # also add a column explaining number of bases in window..\n",
    "\n",
    "    output = np.zeros((eqa_windows.shape[0], arr_loc.shape[1]), dtype=np.int64)\n",
    "    \n",
    "    for ix in range(arr_loc.shape[1]):\n",
    "        \n",
    "        val, windows, counts = allel.stats.window.windowed_statistic(\n",
    "            pos_loc, arr_loc[:, ix], np.sum, windows=eqa_windows)\n",
    "        \n",
    "        output[:, ix] = val\n",
    "        \n",
    "    df = pd.DataFrame(\n",
    "        output, \n",
    "        index=pd.MultiIndex.from_arrays(\n",
    "            [eqa_windows[:, 0], eqa_windows[:, 1]], names=[\"start\", \"stop\"]),\n",
    "        columns=a_labels)\n",
    "    \n",
    "    df.to_csv(path)\n",
    "    \n",
    "    # now totals:\n",
    "    tot = pd.Series(\n",
    "        data=arr_loc.sum(axis=0), index=a_labels, dtype=np.int64, name=\"count\")\n",
    "    \n",
    "    tot.loc[\"total_accessible_bases\"] = loc.sum()\n",
    "    \n",
    "    tot.to_csv(path_total)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils_perf - INFO - full garbage collection released 22.60 MB from 26 reference cycles (threshold: 10.00 MB)\n",
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.34.4.153:52690 remote=tcp://10.34.4.153:41387>\n",
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://10.34.4.153:52694 remote=tcp://10.34.4.153:41387>\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 12.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 5.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 15.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 5.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 10.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 13.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 11.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 13.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - INFO - full garbage collection released 66.35 MB from 10363 reference cycles (threshold: 10.00 MB)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 8.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 11.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 5.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 6.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 7.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n"
     ]
    }
   ],
   "source": [
    "for chrom in chromosomes:    \n",
    "    \n",
    "    g = v3.load_sample_set_calldata(\n",
    "        chrom, sample_set=sample_sets, field=\"GT\")\n",
    "    \n",
    "    ac = allel.GenotypeDaskArray(g).count_alleles_subpops(subpops)\n",
    "    \n",
    "    allelism = {c: da.map_blocks(\n",
    "        get_allelism,\n",
    "        ac[c].values,\n",
    "        chunks=(ac[c].values.chunks[0],),\n",
    "        drop_axis=1).compute() for c in ac.keys()}\n",
    "    \n",
    "    # leverage sum to compute allelism over entire cohort\n",
    "    ac_all = da.stack([ac[c].values for c in ac.keys()], axis=2).sum(axis=2) \n",
    "    allelism[\"all\"] = da.map_blocks(\n",
    "        get_allelism,\n",
    "        ac_all,\n",
    "        chunks=(ac_all.chunks[0],),\n",
    "        drop_axis=1).compute() \n",
    "    \n",
    "    # first look at *all* samples. \n",
    "    # arrays that describe:\n",
    "    # seg in union\n",
    "    # multi in union\n",
    "    is_seg_union = allelism[\"all\"] > 1\n",
    "    is_mta_union = allelism[\"all\"] > 2\n",
    "    \n",
    "    # seg in gamb_col\n",
    "    # multi in gamb_col\n",
    "    is_seg_gambcolu = allelism[\"gamb_colu\"] > 1\n",
    "    is_mta_gambcolu = allelism[\"gamb_colu\"] > 2\n",
    "\n",
    "    # seg in arab\n",
    "    # multi in arab\n",
    "    is_seg_arab = allelism[\"arabiensis\"] > 1\n",
    "    is_mta_arab = allelism[\"arabiensis\"] > 2\n",
    "\n",
    "    # these 4 are mutually exclusive.\n",
    "    # seg in both\n",
    "    is_seg_both = is_seg_arab & is_seg_gambcolu\n",
    "    # priv to gamb_colu\n",
    "    is_priv_gambcolu = is_seg_gambcolu & ~is_seg_arab\n",
    "    # priv to arabarab\n",
    "    is_priv_arab = ~is_seg_gambcolu & is_seg_arab\n",
    "    # fixed diff in both\n",
    "    is_fixed_diff = (~is_seg_gambcolu & ~is_seg_arab) & (is_seg_union)\n",
    "    \n",
    "    pos = v3.load_variants(chrom)\n",
    "    union_mask = v3.load_mask(chrom, \"gamb_colu_arab\").compute()\n",
    "\n",
    "    arr_labels = [\n",
    "        \"is_segregating_gambcoluarab\", \"is_multiallelic_gambcoluarab\",\n",
    "        \"is_segregating_both\", \"is_priv_gambcolu\", \"is_priv_arab\", \"is_fixed_diff\"]\n",
    "    \n",
    "    arr = np.hstack(\n",
    "        [is_seg_union, is_mta_union, \n",
    "         is_seg_both, is_priv_gambcolu, \n",
    "         is_priv_arab, is_fixed_diff])\n",
    "\n",
    "    _ = report_window_values(\n",
    "        positions=pos, loc=union_mask, a=arr, a_labels=arr_labels,\n",
    "        path= f\"../content/tables/snp_discovery/gambcoluarab_{chrom}_windows.csv\",\n",
    "        path_total=f\"../content/tables/snp_discovery/gambcoluarab_{chrom}_tot.csv\")\n",
    "    \n",
    "    # then mask by gamb_colu arab.\n",
    "    # after then sum\n",
    "    # after then compute over windows.\n",
    "    \n",
    "    # could do as loop.\n",
    "    arr_labels = [\"is_segregating\", \"is_multiallelic\"]\n",
    "    for label, arr in zip(\n",
    "        [\"gamb_colu\", \"arab\"], \n",
    "        [np.hstack([is_seg_gambcolu, is_mta_gambcolu]), \n",
    "         np.hstack([is_seg_arab, is_mta_arab])]):\n",
    "    \n",
    "        mask = v3.load_mask(chrom, label).compute()\n",
    "        \n",
    "        _ = report_window_values(\n",
    "            positions=pos, \n",
    "            loc=mask, \n",
    "            a=arr, \n",
    "            a_labels=arr_labels,\n",
    "            path=f\"../content/tables/snp_discovery/{label}_{chrom}_windows.csv\",\n",
    "            path_total=f\"../content/tables/snp_discovery/{label}_{chrom}_tot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<distributed.deploy.adaptive.Adaptive at 0x7fd6d7c2fb10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Retire worker names (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29)\n",
      "distributed.scheduler - INFO - Retire workers {<Worker 'tcp://10.32.152.4:37659', memory: 0, processing: 0>, <Worker 'tcp://10.32.61.6:43103', memory: 0, processing: 0>, <Worker 'tcp://10.32.50.6:45547', memory: 0, processing: 0>, <Worker 'tcp://10.32.56.6:37683', memory: 0, processing: 0>, <Worker 'tcp://10.32.151.4:35483', memory: 0, processing: 0>, <Worker 'tcp://10.32.55.6:45985', memory: 0, processing: 0>, <Worker 'tcp://10.32.59.6:38797', memory: 0, processing: 0>, <Worker 'tcp://10.32.42.6:35077', memory: 0, processing: 0>, <Worker 'tcp://10.32.70.5:42785', memory: 0, processing: 0>, <Worker 'tcp://10.32.66.5:45791', memory: 0, processing: 0>, <Worker 'tcp://10.32.68.5:43315', memory: 0, processing: 0>, <Worker 'tcp://10.32.43.6:38595', memory: 0, processing: 0>, <Worker 'tcp://10.35.208.8:43639', memory: 0, processing: 0>, <Worker 'tcp://10.32.47.5:34113', memory: 0, processing: 0>, <Worker 'tcp://10.32.45.6:41283', memory: 0, processing: 0>, <Worker 'tcp://10.32.49.6:41121', memory: 0, processing: 0>, <Worker 'tcp://10.35.193.10:43779', memory: 0, processing: 0>, <Worker 'tcp://10.32.64.6:42009', memory: 0, processing: 0>, <Worker 'tcp://10.32.113.3:41445', memory: 0, processing: 0>, <Worker 'tcp://10.32.92.7:41671', memory: 0, processing: 0>, <Worker 'tcp://10.32.54.6:42017', memory: 0, processing: 0>, <Worker 'tcp://10.32.51.6:45913', memory: 0, processing: 0>, <Worker 'tcp://10.32.63.6:44047', memory: 0, processing: 0>, <Worker 'tcp://10.32.62.6:36587', memory: 0, processing: 0>, <Worker 'tcp://10.32.155.3:33621', memory: 0, processing: 0>, <Worker 'tcp://10.32.153.4:40237', memory: 0, processing: 0>, <Worker 'tcp://10.32.67.6:43617', memory: 0, processing: 0>, <Worker 'tcp://10.32.46.6:34683', memory: 0, processing: 0>, <Worker 'tcp://10.32.48.6:33085', memory: 0, processing: 0>, <Worker 'tcp://10.32.60.6:43407', memory: 0, processing: 0>}\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.152.4:37659\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.152.4:37659\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.152.4:37659\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.61.6:43103\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.61.6:43103\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.61.6:43103\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.50.6:45547\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.50.6:45547\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.50.6:45547\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.56.6:37683\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.56.6:37683\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.56.6:37683\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.151.4:35483\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.151.4:35483\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.151.4:35483\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.55.6:45985\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.55.6:45985\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.55.6:45985\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.59.6:38797\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.59.6:38797\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.59.6:38797\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.42.6:35077\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.42.6:35077\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.42.6:35077\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.70.5:42785\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.70.5:42785\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.70.5:42785\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.66.5:45791\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.66.5:45791\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.66.5:45791\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.68.5:43315\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.68.5:43315\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.68.5:43315\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.43.6:38595\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.43.6:38595\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.43.6:38595\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.35.208.8:43639\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.35.208.8:43639\n",
      "distributed.core - INFO - Removing comms to tcp://10.35.208.8:43639\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.47.5:34113\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.47.5:34113\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.47.5:34113\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.45.6:41283\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.45.6:41283\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.45.6:41283\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.49.6:41121\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.49.6:41121\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.49.6:41121\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.35.193.10:43779\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.35.193.10:43779\n",
      "distributed.core - INFO - Removing comms to tcp://10.35.193.10:43779\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.64.6:42009\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.64.6:42009\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.64.6:42009\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.113.3:41445\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.113.3:41445\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.113.3:41445\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.92.7:41671\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.92.7:41671\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.92.7:41671\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.54.6:42017\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.54.6:42017\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.54.6:42017\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.51.6:45913\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.51.6:45913\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.51.6:45913\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.63.6:44047\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.63.6:44047\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.63.6:44047\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.62.6:36587\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.62.6:36587\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.62.6:36587\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.155.3:33621\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.155.3:33621\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.155.3:33621\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.153.4:40237\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.153.4:40237\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.153.4:40237\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.67.6:43617\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.67.6:43617\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.67.6:43617\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.46.6:34683\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.46.6:34683\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.46.6:34683\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.48.6:33085\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.48.6:33085\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.48.6:33085\n",
      "distributed.scheduler - INFO - Closing worker tcp://10.32.60.6:43407\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.60.6:43407\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.60.6:43407\n",
      "distributed.scheduler - INFO - Lost all workers\n",
      "distributed.deploy.adaptive - INFO - Retiring workers [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "distributed.scheduler - INFO - Register tcp://10.32.68.5:43315\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.68.5:43315\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.68.5:43315\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.68.5:43315\n",
      "distributed.scheduler - INFO - Lost all workers\n"
     ]
    }
   ],
   "source": [
    "cluster.adapt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
