{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_kubernetes import KubeCluster\n",
    "from dask.distributed import Client, progress\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import zarr\n",
    "import allel\n",
    "import sys\n",
    "import ag3\n",
    "import psutil\n",
    "from humanize import naturalsize\n",
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_group = [\"gamb_colu\", \"arab\", \"gamb_colu_arab\"]\n",
    "module_path = \"../../notebooks/\"\n",
    "n_downsample = 100_000\n",
    "max_allele = 3\n",
    "random_seed = 42\n",
    "n_workers = 10\n",
    "\n",
    "#regions\n",
    "region_3L_free =  '3L', 15000000, 41000000\n",
    "region_3R_free =  '3R', 1, 37000000\n",
    "\n",
    "regions = {\"gamb_colu\" : [region_3L_free, region_3R_free],\n",
    "           \"arab\" : [region_3L_free,],\n",
    "           \"gamb_colu_arab\"  : [region_3L_free,]\n",
    "          }\n",
    "\n",
    "\n",
    "sample_query = {\"gamb_colu\" : \"species_gambcolu_arabiensis == 'gamb_colu'\",\n",
    "                \"arab\" : \"species_gambcolu_arabiensis == 'arabiensis'\",\n",
    "                \"gamb_colu_arab\" : \"species_gambcolu_arabiensis == ('gamb_colu', 'arabiensis', 'intermediate')\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### cloud storage - authentication seems broken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=586241054156-9kst7ltfj66svc342pcn43vp6ta3idin.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&state=jJ9Hqg0XzvuB7y5FMtUUpMExffh9ji&prompt=consent&access_type=offline\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-509ce711c551>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# After running this once, your authentication token should then be cached in `~/.gcs_tokens`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Once you have authenticated, you should comment this out again to avoid re-authenticating.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgcs_browser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcsfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGCSFileSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'malariagen-jupyterhub'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'browser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Use `cache_timeout=0` to prevent object list cache, to avoid recreating map for Zarr consolidated metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fsspec/spec.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Setting _fs_token here causes some static linters to complain.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fs_token_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gcsfs/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, project, access, token, block_size, consistency, cache_timeout, secure_serialize, check_connection, requests_timeout, requester_pays, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequests_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_credentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistings_expiry_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gcsfs/core.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, method)\u001b[0m\n\u001b[1;32m    419\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_connect_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gcsfs/core.py\u001b[0m in \u001b[0;36m_connect_browser\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect_browser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInstalledAppFlow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_client_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mcredentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_console\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google_auth_oauthlib/flow.py\u001b[0m in \u001b[0;36mrun_console\u001b[0;34m(self, authorization_prompt_message, authorization_code_message, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthorization_prompt_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthorization_code_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Data storage, uses about 34 MB\n",
    "output_cloud_zarr_path_template = 'vo_agam_production/ag3_data_paper/{}.pca_umap_input_alleles.zarr'\n",
    "# Writing the PCA data to the cloud will require the appropriate authentication and authorization.\n",
    "\n",
    "import gcsfs\n",
    "# UNCOMMENT THIS TO AUTHENTICATE. YOU ONLY NEED TO RUN THIS ONCE.\n",
    "# After running this once, your authentication token should then be cached in `~/.gcs_tokens`\n",
    "# Once you have authenticated, you should comment this out again to avoid re-authenticating.\n",
    "gcs_browser = gcsfs.GCSFileSystem(project='malariagen-jupyterhub', token='browser')\n",
    "\n",
    "# Use `cache_timeout=0` to prevent object list cache, to avoid recreating map for Zarr consolidated metadata\n",
    "auth_fs = gcsfs.GCSFileSystem(project='malariagen-jupyterhub', token='cache', cache_timeout=0)\n",
    "\n",
    "for sp in species_group:\n",
    "\n",
    "    # Check that the output's Zarr metadata file is not already on the cloud.\n",
    "    # We don't want to accidentally overwrite or delete existing data, which might have been used in downstream analysis.\n",
    "    # We don't simply check for the existence of the Zarr file here (i.e. output_cloud_zarr_path),\n",
    "    # We might want to re-run the first parts of this notebook again,\n",
    "    # so the Zarr store might legitimately exist but be incomplete.\n",
    "    # The Zarr store is not considered complete until the Zarr metadata file (.zmetadata) is present and correct.\n",
    "    # The final part of this notebook includes steps to create the Zarr metadata file and then validate it.\n",
    "\n",
    "    output_cloud_zarr_metadata_path = f'{output_cloud_zarr_path_template.format(sp)}/.zmetadata'\n",
    "    print(f'Checking for {output_cloud_zarr_metadata_path}')\n",
    "    assert not auth_fs.exists(output_cloud_zarr_metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2 GB used, 14.2 GB available, 15.8 GB total\n"
     ]
    }
   ],
   "source": [
    "def show_memory():\n",
    "      vm = psutil.virtual_memory()\n",
    "      print(f\"{naturalsize(vm.used)} used, {naturalsize(vm.available)} available, {naturalsize(vm.total)} total\")\n",
    "show_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to return the genotype data for a given chromosome region, for a given sample set.\n",
    "# Use the specified intake catalog.\n",
    "def load_genotype_calldata(cat, sample_set, chrom_arm, region_slice_obj):\n",
    "    print('- load_genotype_calldata', sample_set, chrom_arm, region_slice_obj)\n",
    "    zarr_data = cat.ag3.snp_genotypes(sample_set=sample_set).to_zarr()\n",
    "    return da.from_zarr(zarr_data[chrom_arm][\"calldata\"][\"GT\"])[region_slice_obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "# Define a function to count the presence of each allele in a given array of genotypes (samples * variants * alleles)\n",
    "    # and return an array of allele counts with a row for each possible allele (limited by max_allele) for each sample, and column for each variant\n",
    "    @staticmethod\n",
    "    @numba.njit(numba.int8[:, :](numba.int8[:, :, :], numba.int8), nogil=True)\n",
    "    def numpy_genotype_tensor_to_allele_counts_melt(gt, max_allele):\n",
    "        # Create an array of zeros (for defaults) with the same number of colums (variants) as the genotype array but a row for each allele, for each sample\n",
    "        out = np.zeros((gt.shape[0] * (max_allele + 1), gt.shape[1]), dtype=np.int8)\n",
    "        # For each row (sample) in the genotype array\n",
    "        for i in range(gt.shape[0]):\n",
    "            # For each column (variant) in the genotype array\n",
    "            for j in range(gt.shape[1]):\n",
    "                # For each allele in the genotype array \n",
    "                for k in range(gt.shape[2]):\n",
    "                    allele = gt[i, j, k]\n",
    "                    # If the value in the genotype array at this row and colum and 3rd dimension (i.e. the allele value) is between 0 and max_allele  \n",
    "                    if 0 <= allele <= max_allele:\n",
    "                        # Increment the value of the `out` array at the row corresponding to this allele for this sample, at the corresponding variant column\n",
    "                        out[(i * (max_allele + 1)) + allele, j] += 1\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the only way I could get the code to work is to define this ^^ as Util otherwise the map blocks code doesn't run - why is this? - fix with Nick's help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the above function chunk-wise\n",
    "def dask_genotype_tensor_to_allele_counts_melt(gt, max_allele):\n",
    "    # Determine output chunks - change axis 0; preserve axis 1; drop axis 2.\n",
    "    dim0_chunks = tuple(np.array(gt.chunks[0]) * (max_allele + 1))\n",
    "    chunks = (dim0_chunks, gt.chunks[1])\n",
    "    \n",
    "    return gt.map_blocks(\n",
    "        Util.numpy_genotype_tensor_to_allele_counts_melt,\n",
    "        max_allele=max_allele,\n",
    "        chunks=chunks,\n",
    "        dtype=\"i1\",\n",
    "        drop_axis=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can this be improved with dask? - Nick?\n",
    "def ld_prune(gn, size=500, step=200, threshold=.1, n_iter=1):\n",
    "    for i in range(n_iter):\n",
    "        loc_unlinked = allel.locate_unlinked(gn, size=size, step=step, threshold=threshold)\n",
    "        n = np.count_nonzero(loc_unlinked)\n",
    "        n_remove = gn.shape[0] - n\n",
    "        print('iteration', i+1, 'retaining', n, 'removing', n_remove, 'variants')\n",
    "        gn = gn.compress(loc_unlinked, axis=0)\n",
    "    return gn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Clear task state\n",
      "/opt/conda/lib/python3.7/site-packages/distributed/dashboard/core.py:72: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n",
      "distributed.scheduler - INFO -   Scheduler at:    tcp://10.33.40.6:38307\n",
      "distributed.scheduler - INFO -   dashboard at:                    :36581\n",
      "distributed.scheduler - INFO - Receive client connection: Client-ab8d6fc8-08b8-11eb-82d5-12daa7a5e979\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/c.clarkson@liverpool.ac.uk/proxy/36581/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register tcp://10.33.21.3:42761\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.21.3:42761\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.234.3:46287\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.234.3:46287\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.33.19.3:43861\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.19.3:43861\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.33.55.3:35049\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.55.3:35049\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.33.9.3:41913\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.9.3:41913\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.218.3:45405\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.218.3:45405\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.33.59.3:38633\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.59.3:38633\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.248.3:40299\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.248.3:40299\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.33.3.3:35203\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.3.3:35203\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.33.50.3:40759\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.33.50.3:40759\n",
      "distributed.core - INFO - Starting established connection\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    }
   ],
   "source": [
    "# cluster setup\n",
    "cluster = KubeCluster()\n",
    "cluster.scale_up(n_workers)\n",
    "\n",
    "# dask client setup\n",
    "client = Client(cluster)\n",
    "print(cluster.dashboard_link)\n",
    "\n",
    "# grab data from release\n",
    "v3 = ag3.release_data()\n",
    "\n",
    "sample_sets = v3.all_wild_sample_sets\n",
    "\n",
    "# load_metadata\n",
    "metadata = v3.load_sample_set_metadata(sample_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## when I get permissions to write to cloud un-hash the zarr writing bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_allele_counts(selection):\n",
    "    print(\"building allele counts for\", selection)\n",
    "    \n",
    "    sample_loc = metadata.eval(sample_query[selection]).values\n",
    "    assert sample_loc.sum() > 0, \"Must select >0 samples\"\n",
    "\n",
    "    genotypes = []\n",
    "    site_filters = []\n",
    "    \n",
    "    \n",
    "    for reg in (regions[selection]):      \n",
    "        chrom, start, stop = reg\n",
    "        print(chrom, start, stop)\n",
    "\n",
    "        pos = allel.SortedIndex(v3.load_variants(chrom))\n",
    "        ix = pos.locate_range(start, stop)\n",
    "        gt = v3.load_sample_set_calldata(chrom, sample_set=sample_sets)[ix]\n",
    "        mask = v3.load_mask(chrom, selection)[ix]\n",
    "        g = da.compress(sample_loc, gt, axis=1)\n",
    "\n",
    "        genotypes.append(g)\n",
    "        site_filters.append(mask)\n",
    "\n",
    "    genotype_data = da.concatenate(genotypes, axis=0)\n",
    "    site_filters_data = da.concatenate(site_filters, axis=0)\n",
    "\n",
    "    melted_allele_counts = dask_genotype_tensor_to_allele_counts_melt(gt=genotype_data, max_allele=max_allele)\n",
    "\n",
    "    # Get the number of genotyped samples\n",
    "    number_of_samples = genotype_data.shape[1]\n",
    "    print(\"Number of samples\", number_of_samples)\n",
    "\n",
    "    # Sum the allele counts\n",
    "    allele_count_sums = da.sum(melted_allele_counts, axis=1, dtype='int16')\n",
    "\n",
    "    # Determine which alleles meet the criteria, and record as a Boolean array.\n",
    "    loc_midfreq_alleles = (allele_count_sums >= 2) & (allele_count_sums <= ((number_of_samples * 2) - 2))\n",
    "\n",
    "    # Transform the Boolean site_filter index into the same space as the melted allele counts.\n",
    "    loc_accessible = da.repeat(site_filters_data, max_allele + 1) # 4 alleles\n",
    "\n",
    "    # Check that loc_accessible is the same shape as loc_midfreq_alleles\n",
    "    assert loc_accessible.shape == loc_midfreq_alleles.shape\n",
    "\n",
    "    # Determine the corresponding array indices for all of the mid-frequency alleles that are accessible\n",
    "    # We use the '&' to choose sites that meet the critera AND are accessible.\n",
    "    midfreq_alleles_as_indices = da.nonzero(loc_midfreq_alleles & loc_accessible)[0]\n",
    "\n",
    "    # Compute (and bring into client memory) the midfreq_alleles_as_indices\n",
    "    ix_select = midfreq_alleles_as_indices.compute()\n",
    "    \n",
    "    # Set/reset the random seed used for random variant selection\n",
    "    # to ensure that we always select the same set of random variants\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Randomly choose `n_downsample_variants` items from the array of accessible mid-frequency allele indices\n",
    "    downsampled_site_indices = np.random.choice(\n",
    "        ix_select, \n",
    "        size=n_downsample, \n",
    "        replace=False)\n",
    "\n",
    "    # Sort the indices to allow contiguous parsing\n",
    "    downsampled_site_indices.sort()\n",
    "\n",
    "    # From the melted_allele_counts array, take the corresponding indices\n",
    "    downsampled_allele_counts = da.take(melted_allele_counts, downsampled_site_indices, axis=0)   \n",
    "    computed_downsampled_allele_counts = downsampled_allele_counts.compute()\n",
    "    \n",
    "    #prune\n",
    "    pruned_downsampled_allele_counts = ld_prune(computed_downsampled_allele_counts)\n",
    "    \n",
    "#     # finally save to zarr...\n",
    "#     print('saving to zarr')\n",
    "#     output_cloud_zarr_path = output_cloud_zarr_path_template.format(selection)\n",
    "#     print('Will attempt to store at:', output_cloud_zarr_path)\n",
    "\n",
    "#     # Sometimes errors with `overwrite=True`, sometimes errors without, when dir not exist\n",
    "#     # Keep the zarr_store for zarr.consolidate_metadata(zarr_store)\n",
    "#     zarr_store = auth_fs.get_mapper(output_cloud_zarr_path)\n",
    "#     zarr_group = zarr.group(zarr_store)\n",
    "\n",
    "#     # Check the data type\n",
    "#     print('type(pruned_downsampled_allele_counts):', type(pruned_downsampled_allele_counts))\n",
    "    \n",
    "#     # overwrite=True, otherwise `ValueError: path 'allele_counts_pca_ready' contains an array`\n",
    "#     zarr_group.create_dataset(\"allele_counts_pca_ready\", data=pruned_downsampled_allele_counts, overwrite=True)\n",
    "    \n",
    "#     # Check the stored data has all its chunks initialized\n",
    "#     assert zarr_group['allele_counts_pca_ready'].nchunks_initialized == zarr_group['allele_counts_pca_ready'].nchunks\n",
    "    \n",
    "#     # Check the store contains the expected arrays\n",
    "#     assert 'allele_counts_pca_ready/.zarray' in zarr_store\n",
    "    \n",
    "#     # Consolidate the Zarr metatdata\n",
    "#     zarr.consolidate_metadata(zarr_store)\n",
    "    \n",
    "#     # Check the consolidated Zarr metadata\n",
    "#     zarr_consolidated_metadata = zarr.open_consolidated(zarr_store)\n",
    "#     assert list(zarr_consolidated_metadata.keys()) == ['allele_counts_pca_ready']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building allele counts for gamb_colu\n",
      "3L 15000000 41000000\n",
      "Number of samples 2415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 12.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 14.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 retaining 81239 removing 18761 variants\n",
      "saving to zarr\n"
     ]
    }
   ],
   "source": [
    "for s in species_group:\n",
    "    build_allele_counts(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Scheduler closing...\n",
      "distributed.scheduler - INFO - Scheduler closing all comms\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.33.19.3:43861\n",
      "distributed.core - INFO - Removing comms to tcp://10.33.19.3:43861\n",
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/distributed/utils.py\", line 663, in log_errors\n",
      "    yield\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/distributed/client.py\", line 1296, in _close\n",
      "    await gen.with_timeout(timedelta(seconds=2), list(coroutines))\n",
      "concurrent.futures._base.CancelledError\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/distributed/utils.py\", line 663, in log_errors\n",
      "    yield\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/distributed/client.py\", line 1025, in _reconnect\n",
      "    await self._close()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/distributed/client.py\", line 1296, in _close\n",
      "    await gen.with_timeout(timedelta(seconds=2), list(coroutines))\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
