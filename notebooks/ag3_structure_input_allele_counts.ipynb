{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_kubernetes import KubeCluster\n",
    "from dask.distributed import Client, progress\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import zarr\n",
    "import allel\n",
    "import sys\n",
    "import ag3\n",
    "import psutil\n",
    "from humanize import naturalsize\n",
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_group=[\"gamb_colu\", \"arab\", \"gamb_colu_arab\"]\n",
    "module_path = \"../../notebooks/\"\n",
    "n_downsample = 100_000\n",
    "max_allele = 3\n",
    "random_seed = 42\n",
    "n_workers = 10\n",
    "\n",
    "#regions\n",
    "region_3L_free =  '3L', 15000000, 41000000\n",
    "region_3R_free =  '3R', 1, 37000000\n",
    "\n",
    "regions = {\"gamb_colu\" : [region_3L_free, region_3R_free],\n",
    "           \"arab\" : region_3L_free,\n",
    "           \"gamb_colu_arab\"  : region_3L_free\n",
    "          }\n",
    "\n",
    "\n",
    "#This is correct - gamb_colu_arab includes ALL samples\n",
    "sample_query = {\"gamb_colu\" : \"species_gambcolu_arabiensis == 'gamb_colu'\",\n",
    "                \"arab\" : \"species_gambcolu_arabiensis == 'arabiensis'\",\n",
    "                \"gamb_colu_arab\" : \"species_gambcolu_arabiensis != 'NA'\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6 GB used, 13.9 GB available, 15.8 GB total\n"
     ]
    }
   ],
   "source": [
    "def show_memory():\n",
    "      vm = psutil.virtual_memory()\n",
    "      print(f\"{naturalsize(vm.used)} used, {naturalsize(vm.available)} available, {naturalsize(vm.total)} total\")\n",
    "show_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to return the genotype data for a given chromosome region, for a given sample set.\n",
    "# Use the specified intake catalog.\n",
    "def load_genotype_calldata(cat, sample_set, chrom_arm, region_slice_obj):\n",
    "    print('- load_genotype_calldata', sample_set, chrom_arm, region_slice_obj)\n",
    "    zarr_data = cat.ag3.snp_genotypes(sample_set=sample_set).to_zarr()\n",
    "    return da.from_zarr(zarr_data[chrom_arm][\"calldata\"][\"GT\"])[region_slice_obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Util:\n",
    "# Define a function to count the presence of each allele in a given array of genotypes (samples * variants * alleles)\n",
    "    # and return an array of allele counts with a row for each possible allele (limited by max_allele) for each sample, and column for each variant\n",
    "    @staticmethod\n",
    "    @numba.njit(numba.int8[:, :](numba.int8[:, :, :], numba.int8), nogil=True)\n",
    "    def numpy_genotype_tensor_to_allele_counts_melt(gt, max_allele):\n",
    "        # Create an array of zeros (for defaults) with the same number of colums (variants) as the genotype array but a row for each allele, for each sample\n",
    "        out = np.zeros((gt.shape[0] * (max_allele + 1), gt.shape[1]), dtype=np.int8)\n",
    "        # For each row (sample) in the genotype array\n",
    "        for i in range(gt.shape[0]):\n",
    "            # For each column (variant) in the genotype array\n",
    "            for j in range(gt.shape[1]):\n",
    "                # For each allele in the genotype array \n",
    "                for k in range(gt.shape[2]):\n",
    "                    allele = gt[i, j, k]\n",
    "                    # If the value in the genotype array at this row and colum and 3rd dimension (i.e. the allele value) is between 0 and max_allele  \n",
    "                    if 0 <= allele <= max_allele:\n",
    "                        # Increment the value of the `out` array at the row corresponding to this allele for this sample, at the corresponding variant column\n",
    "                        out[(i * (max_allele + 1)) + allele, j] += 1\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the only way I could get the code to work is to define this ^^ as Util otherwise the map blocks code doesn't run - why is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the above function chunk-wise\n",
    "def dask_genotype_tensor_to_allele_counts_melt(gt, max_allele):\n",
    "    # Determine output chunks - change axis 0; preserve axis 1; drop axis 2.\n",
    "    dim0_chunks = tuple(np.array(gt.chunks[0]) * (max_allele + 1))\n",
    "    chunks = (dim0_chunks, gt.chunks[1])\n",
    "    \n",
    "    return gt.map_blocks(\n",
    "        Util.numpy_genotype_tensor_to_allele_counts_melt,\n",
    "        max_allele=max_allele,\n",
    "        chunks=chunks,\n",
    "        dtype=\"i1\",\n",
    "        drop_axis=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can this be improved with dask?\n",
    "def ld_prune(gn, size=500, step=200, threshold=.1, n_iter=1):\n",
    "    for i in range(n_iter):\n",
    "        loc_unlinked = allel.locate_unlinked(gn, size=size, step=step, threshold=threshold)\n",
    "        n = np.count_nonzero(loc_unlinked)\n",
    "        n_remove = gn.shape[0] - n\n",
    "        print('iteration', i+1, 'retaining', n, 'removing', n_remove, 'variants')\n",
    "        gn = gn.compress(loc_unlinked, axis=0)\n",
    "    return gn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at:   tcp://10.32.38.56:46693\n",
      "distributed.scheduler - INFO -   dashboard at:                     :8787\n",
      "distributed.scheduler - INFO - Receive client connection: Client-ab1d3d28-026b-11eb-83fc-d68730c52af1\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/c.clarkson@liverpool.ac.uk/proxy/8787/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register tcp://10.32.75.5:38031\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.75.5:38031\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.34.5:32803\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.34.5:32803\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.88.5:33909\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.88.5:33909\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.65.5:40837\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.65.5:40837\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.81.3:42111\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.81.3:42111\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.78.5:43639\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.78.5:43639\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.96.5:36709\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.96.5:36709\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.67.5:41559\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.67.5:41559\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.89.5:34753\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.89.5:34753\n",
      "distributed.core - INFO - Starting established connection\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n",
      "distributed.scheduler - INFO - Register tcp://10.32.83.5:37519\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.83.5:37519\n",
      "distributed.core - INFO - Starting established connection\n",
      "Exception ignored in: <finalize object at 0x7fbff0b48fb0; dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/weakref.py\", line 572, in __call__\n",
      "    return info.func(*info.args, **(info.kwargs or {}))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/dask_kubernetes/core.py\", line 708, in _cleanup_resources\n",
      "    namespace, label_selector=format_labels(labels)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 13463, in list_namespaced_service\n",
      "    (data) = self.list_namespaced_service_with_http_info(namespace, **kwargs)  # noqa: E501\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kubernetes/client/api/core_v1_api.py\", line 13565, in list_namespaced_service_with_http_info\n",
      "    collection_formats=collection_formats)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 345, in call_api\n",
      "    _preload_content, _request_timeout)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 176, in __call_api\n",
      "    _request_timeout=_request_timeout)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kubernetes/client/api_client.py\", line 366, in request\n",
      "    headers=headers)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 241, in GET\n",
      "    query_params=query_params)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kubernetes/client/rest.py\", line 231, in request\n",
      "    raise ApiException(http_resp=r)\n",
      "kubernetes.client.rest.ApiException: (403)\n",
      "Reason: Forbidden\n",
      "HTTP response headers: HTTPHeaderDict({'Audit-Id': 'de0b5643-9b9c-42c8-9886-c62cc08390b5', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Tue, 29 Sep 2020 15:52:00 GMT', 'Content-Length': '307'})\n",
      "HTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"services is forbidden: User \\\"system:serviceaccount:malariagen:daskkubernetes\\\" cannot list resource \\\"services\\\" in API group \\\"\\\" in the namespace \\\"malariagen\\\"\",\"reason\":\"Forbidden\",\"details\":{\"kind\":\"services\"},\"code\":403}\n",
      "\n",
      "\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7fbff0a7b9d0>\n",
      "Unclosed connector\n",
      "connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x7fbfb893e360>, 12807665.865987085), (<aiohttp.client_proto.ResponseHandler object at 0x7fbfb9034e50>, 12807665.869185492), (<aiohttp.client_proto.ResponseHandler object at 0x7fbfb884cde0>, 12807665.870300738), (<aiohttp.client_proto.ResponseHandler object at 0x7fbfb884c130>, 12807665.870938657), (<aiohttp.client_proto.ResponseHandler object at 0x7fbfb893eec0>, 12807665.871574253), (<aiohttp.client_proto.ResponseHandler object at 0x7fbfc22df590>, 12807665.872469492), (<aiohttp.client_proto.ResponseHandler object at 0x7fbfb893e670>, 12807665.873114431), (<aiohttp.client_proto.ResponseHandler object at 0x7fbfc1381c90>, 12807665.873694684), (<aiohttp.client_proto.ResponseHandler object at 0x7fbfb884c3d0>, 12807665.875265071), (<aiohttp.client_proto.ResponseHandler object at 0x7fbfc1381a60>, 12807665.875867931)]']\n",
      "connector: <aiohttp.connector.TCPConnector object at 0x7fbff0a7b750>\n"
     ]
    }
   ],
   "source": [
    "# cluster setup\n",
    "cluster = KubeCluster()\n",
    "cluster.scale_up(n_workers)\n",
    "\n",
    "# dask client setup\n",
    "client = Client(cluster)\n",
    "print(cluster.dashboard_link)\n",
    "\n",
    "# grab data from release\n",
    "v3 = ag3.release_data()\n",
    "\n",
    "sample_sets = v3.all_wild_sample_sets\n",
    "\n",
    "# load_metadata\n",
    "metadata = v3.load_sample_set_metadata(sample_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register tcp://10.32.81.2:36009\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.81.2:36009\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    }
   ],
   "source": [
    "def build_allele_counts(selection):\n",
    "    print(\"building allele counts for\", selection)\n",
    "    \n",
    "    sample_loc = metadata.eval(sample_query[selection]).values\n",
    "    assert sample_loc.sum() > 0, \"Must select >0 samples\"\n",
    "\n",
    "    genotypes = []\n",
    "    site_filters = []\n",
    "    \n",
    "    chrom, start, stop = regions[selection]\n",
    "    print(chrom, start, stop)\n",
    "    #     \n",
    "    pos = allel.SortedIndex(v3.load_variants(chrom))\n",
    "    ix = pos.locate_range(start, stop)\n",
    "    gt = v3.load_sample_set_calldata(chrom, sample_set=sample_sets)[ix]\n",
    "    mask = v3.load_mask(chrom, selection)[ix]\n",
    "    g = da.compress(sample_loc, gt, axis=1)\n",
    "\n",
    "    genotypes.append(g)\n",
    "    site_filters.append(mask)\n",
    "\n",
    "    genotype_data = da.concatenate(genotypes, axis=0)\n",
    "    site_filters_data = da.concatenate(site_filters, axis=0)\n",
    "\n",
    "    melted_allele_counts = dask_genotype_tensor_to_allele_counts_melt(gt=genotype_data, max_allele=max_allele)\n",
    "    \n",
    "    # Get the number of genotyped samples\n",
    "    number_of_samples = genotype_data.shape[1]\n",
    "    print(\"Number of samples\", number_of_samples)\n",
    "\n",
    "    # Sum the allele counts\n",
    "    allele_count_sums = da.sum(melted_allele_counts, axis=1, dtype='int16')\n",
    "\n",
    "    # Determine which alleles meet the criteria, and record as a Boolean array.\n",
    "    loc_midfreq_alleles = (allele_count_sums >= 2) & (allele_count_sums <= ((number_of_samples * 2) - 2))\n",
    "\n",
    "    # Transform the Boolean site_filter index into the same space as the melted allele counts.\n",
    "    loc_accessible = da.repeat(site_filters_data, max_allele + 1) # 4 alleles\n",
    "\n",
    "    # Check that loc_accessible is the same shape as loc_midfreq_alleles\n",
    "    assert loc_accessible.shape == loc_midfreq_alleles.shape\n",
    "    \n",
    "    # Determine the corresponding array indices for all of the mid-frequency alleles that are accessible\n",
    "    # We use the '&' to choose sites that meet the critera AND are accessible.\n",
    "    midfreq_alleles_as_indices = da.nonzero(loc_midfreq_alleles & loc_accessible)[0]\n",
    "\n",
    "    # Compute (and bring into client memory) the midfreq_alleles_as_indices\n",
    "    ix_select = midfreq_alleles_as_indices.compute()\n",
    "    \n",
    "    # Set/reset the random seed used for random variant selection\n",
    "    # to ensure that we always select the same set of random variants\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Randomly choose `n_downsample_variants` items from the array of accessible mid-frequency allele indices\n",
    "    downsampled_site_indices = np.random.choice(\n",
    "        ix_select, \n",
    "        size=n_downsample, \n",
    "        replace=False)\n",
    "\n",
    "    # Sort the indices to allow contiguous parsing\n",
    "    downsampled_site_indices.sort()\n",
    "\n",
    "    # From the melted_allele_counts array, take the corresponding indices\n",
    "    downsampled_allele_counts = da.take(melted_allele_counts, downsampled_site_indices, axis=0)   \n",
    "    computed_downsampled_allele_counts = downsampled_allele_counts.compute()\n",
    "    \n",
    "    #prune\n",
    "    pruned_downsampled_allele_counts = ld_prune(computed_downsampled_allele_counts)\n",
    "    \n",
    "    # finally save to zarr...\n",
    "    print('saving to zarr')\n",
    "    z = zarr.ZipStore(\"../data/allele_counts_for_pca_umap/\"+selection+\".pca_umap_input_alleles.zarr.zip\")\n",
    "    zg = zarr.group(z)\n",
    "    zg.create_dataset(\"allele_counts_pca_ready\", data=pruned_downsampled_allele_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building allele counts for arab\n",
      "3L 15000000 41000000\n",
      "Number of samples 368\n",
      "iteration 1 retaining 40862 removing 59138 variants\n"
     ]
    }
   ],
   "source": [
    "for s in ['arab',]:\n",
    "    build_allele_counts(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Scheduler closing...\n",
      "distributed.scheduler - INFO - Scheduler closing all comms\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.65.5:40837\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.65.5:40837\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.34.5:32803\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.34.5:32803\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.32.75.5:38031\n",
      "distributed.core - INFO - Removing comms to tcp://10.32.75.5:38031\n",
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/distributed/utils.py\", line 663, in log_errors\n",
      "    yield\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/distributed/client.py\", line 1296, in _close\n",
      "    await gen.with_timeout(timedelta(seconds=2), list(coroutines))\n",
      "concurrent.futures._base.CancelledError\n",
      "distributed.utils - ERROR - \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/distributed/utils.py\", line 663, in log_errors\n",
      "    yield\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/distributed/client.py\", line 1025, in _reconnect\n",
      "    await self._close()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/distributed/client.py\", line 1296, in _close\n",
      "    await gen.with_timeout(timedelta(seconds=2), list(coroutines))\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test using 'arab' - delete once function working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = 'arab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster setup\n",
    "cluster = KubeCluster()\n",
    "cluster.scale_up(n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask client setup\n",
    "client = Client(cluster)\n",
    "print(cluster.dashboard_link)\n",
    "\n",
    "# grab data from release\n",
    "v3 = ag3.release_data()\n",
    "\n",
    "sample_sets = v3.all_wild_sample_sets\n",
    "\n",
    "# load_metadata\n",
    "metadata = v3.load_sample_set_metadata(sample_sets)\n",
    "\n",
    "sample_loc = metadata.eval(sample_query[selection]).values\n",
    "assert sample_loc.sum() > 0, \"Must select >0 samples\"\n",
    "\n",
    "genotypes = []\n",
    "site_filters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrom, start, stop = regions[selection]\n",
    "print(chrom, start, stop)\n",
    "#     \n",
    "pos = allel.SortedIndex(v3.load_variants(chrom))\n",
    "ix = pos.locate_range(start, stop)\n",
    "gt = v3.load_sample_set_calldata(chrom, sample_set=sample_sets)[ix]\n",
    "mask = v3.load_mask(chrom, selection)[ix]\n",
    "g = da.compress(sample_loc, gt, axis=1)\n",
    "\n",
    "genotypes.append(g)\n",
    "site_filters.append(mask)\n",
    "\n",
    "genotype_data = da.concatenate(genotypes, axis=0)\n",
    "site_filters_data = da.concatenate(site_filters, axis=0)\n",
    "\n",
    "melted_allele_counts = dask_genotype_tensor_to_allele_counts_melt(gt=genotype_data, max_allele=max_allele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of genotyped samples\n",
    "number_of_samples = genotype_data.shape[1]\n",
    "print(\"Number of samples\", number_of_samples)\n",
    "\n",
    "# Sum the allele counts\n",
    "allele_count_sums = da.sum(melted_allele_counts, axis=1, dtype='int16')\n",
    "\n",
    "# Determine which alleles meet the criteria, and record as a Boolean array.\n",
    "loc_midfreq_alleles = (allele_count_sums >= 2) & (allele_count_sums <= ((number_of_samples * 2) - 2))\n",
    "\n",
    "# Transform the Boolean site_filter index into the same space as the melted allele counts.\n",
    "loc_accessible = da.repeat(site_filters_data, max_allele + 1) # 4 alleles\n",
    "\n",
    "# Check that loc_accessible is the same shape as loc_midfreq_alleles\n",
    "assert loc_accessible.shape == loc_midfreq_alleles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the corresponding array indices for all of the mid-frequency alleles that are accessible\n",
    "# We use the '&' to choose sites that meet the critera AND are accessible.\n",
    "midfreq_alleles_as_indices = da.nonzero(loc_midfreq_alleles & loc_accessible)[0]\n",
    "\n",
    "# Compute (and bring into client memory) the midfreq_alleles_as_indices\n",
    "ix_select = midfreq_alleles_as_indices.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set/reset the random seed used for random variant selection\n",
    "# to ensure that we always select the same set of random variants\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Randomly choose `n_downsample_variants` items from the array of accessible mid-frequency allele indices\n",
    "downsampled_site_indices = np.random.choice(\n",
    "    ix_select, \n",
    "    size=n_downsample, \n",
    "    replace=False)\n",
    "\n",
    "# Sort the indices to allow contiguous parsing\n",
    "downsampled_site_indices.sort()\n",
    "\n",
    "# From the melted_allele_counts array, take the corresponding indices\n",
    "downsampled_allele_counts = da.take(melted_allele_counts, downsampled_site_indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_downsampled_allele_counts = downsampled_allele_counts.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ld_prune(gn, size=500, step=200, threshold=.1, n_iter=1):\n",
    "    for i in range(n_iter):\n",
    "        loc_unlinked = allel.locate_unlinked(gn, size=size, step=step, threshold=threshold)\n",
    "        n = np.count_nonzero(loc_unlinked)\n",
    "        n_remove = gn.shape[0] - n\n",
    "        print('iteration', i+1, 'retaining', n, 'removing', n_remove, 'variants')\n",
    "        gn = gn.compress(loc_unlinked, axis=0)\n",
    "    return gn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_downsampled_allele_counts = ld_prune(computed_downsampled_allele_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally save to zarr...\n",
    "z = zarr.ZipStore(\"../data/allele_counts_for_pca_umap/\"+selection+\".pca_umap_input_alleles.zarr.zip\")\n",
    "zg = zarr.group(z)\n",
    "zg.create_dataset(\"allele_counts_pca_ready\", data=pruned_downsampled_allele_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_downsampled_allele_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
